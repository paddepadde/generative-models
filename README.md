# Generative Models

This repository contains a collection of _PyTorch_ implementations of popular _generative deep neural network models_ (VAEs and GANs) trained on a set of different tutorial datasets in an unsupervised learning approach. The models learn to generate images that are similar to the training data and can, thus, be used to generate new unseen samples. 


## Content

Implementaitons are split into different Jupyer notebookes based on the dataset and used method. 
The results are presented in the corresponding notebooks. The notebooks also contains intructions for downloading the datasets if you want to run the experiments yourself. 

In the current version the repository contains generative model for the following datasets:

* __MNIST Dataset__: The classic tutorial dataset. Contains data in form of 60'000 grayscale images of handwritten digits (0-9). Tested with both VAE (`mnist_vae.ipynb`) and GAN (`mnist_vae.ipynb`).

* __CelebA Dataset__: Contains dataset in form of more than 100'000 RGB human portrait photos.   
Tested with both VAE (`celeba_vae.ipynb`) and GAN (`celeb_gan.ipynb`). Results are far superior with the GAN model. 

* __Standford Car Dataset__: Contains 16'000 RGB car images from various different car models.  
Tested with GAN (`car_gan.ipynb`).


## Generative Adversarial Networks (GANs)

The GAN architecture was introduced by Goodfellow et al. (2014) ([https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)) consists of two neural networks: a _Generator_ network, that learns to generate images from random noise, and a _Discriminator_ network, that tries to differentiate between the real data (training images) and the fake images (generated from the generator).   

The training process can be summarized as a game in which the generator tries to produce images, that are similar to the generator, in order to trick the discriminator into thinking the generated images are real. 

![GAN Training](https://imgur.com/cMQQqmV.png)


The animation shows the type of images that are generated as the model is trained. As more the model is trained on more real images, the generated images start to look more realistic and similar to the training images. 

![Animation of Training](https://imgur.com/DDT3fE8.gif)

__References:__

* Original GAN paper (Goodfellow et al., 2014): [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)
* DCGAN Paper (Radford et al., 2015): [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434) 
* PyTorch DCGAN Tutorial: [https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
* Google GAN Introduction: [https://developers.google.com/machine-learning/gan](https://developers.google.com/machine-learning/gan)

## Variational Autoencoders (VAEs)

VAEs are an other popular architecture for generative neural network models. The architecture consists of a concatination of an encoder-network and an decoder-network (Kingma, 2013) [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114).  
The enoder network uses the training images as input and learns the parameters of a lower-dimensional probability distribution, that can be used to sample a latent vector z from.  
The decoder network aims then aims to reconstruct the original input image from this sampled vector z. 

By forcing the learned probability distribution to stay close to N(0, I) new unseen data can be generated by sampling from N(0, I) and using this as input for the decoder network.

__References__

* Original VAE paper (Kingma et al. 2013): [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114).
* VAE Tutorial (Doersch): [https://arxiv.org/abs/1606.05908](https://arxiv.org/abs/1606.05908)

